\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}

\begin{document}

\title{Br41n.io Hackathon 2025 G25 - Enhanced SSVEP Classification Using Combined Machine Learning Approaches}

\author{\IEEEauthorblockN{Yunseok Choi\textsuperscript{1}, Daria Elagina\textsuperscript{2}, Mohamed A. Elsherif\textsuperscript{3}, db\textsuperscript{4}}
\IEEEauthorblockA{\textsuperscript{1}Email: contact@yunseokchoi.com}
\IEEEauthorblockA{\textsuperscript{2}Email: elagina\_daria@gmx.de}
\IEEEauthorblockA{\textsuperscript{3}Email: dr\_mohamed\_elsherif@yahoo.com}
\IEEEauthorblockA{\textsuperscript{4}Email: 4dprema@gmail.com}
\and
\IEEEauthorblockN{Ibrahim Koukash\textsuperscript{5}, Burhanuddin Godhra\textsuperscript{6}, Sonu Yadav\textsuperscript{7}, Hozan Mohammadi\textsuperscript{8}}
\IEEEauthorblockA{\textsuperscript{5}Email: koukashibrahim@gmail.com}
\IEEEauthorblockA{\textsuperscript{6}Email: burhanuddinmustafa966@gmail.com}
\IEEEauthorblockA{\textsuperscript{7}Email: sonu.yadav1999@gmail.com}
\IEEEauthorblockA{\textsuperscript{8}Email: dr.hozan.mo@gmail.com}
}

\maketitle

\begin{abstract}
Steady-State Visual Evoked Potential (SSVEP) is a widely used paradigm in Brain-Computer Interfaces (BCIs) due to its high signal-to-noise ratio and relatively simple experimental setup. However, classification performance can vary significantly between subjects, presenting challenges for practical applications. This paper presents a comparative analysis of traditional signal processing methods (Filter Bank Canonical Correlation Analysis, FBCCA) and machine learning approaches trained on combined datasets. Our results demonstrate that while FBCCA achieves 95-97\% accuracy for subjects with strong SSVEP responses, performance drops to 68-72\% for subjects with atypical neural responses. In contrast, our improved machine learning approach achieves 100\% classification accuracy across all subjects with enhanced feature extraction and optimized model parameters. We analyze the factors contributing to this performance gap and propose guidelines for SSVEP-based BCI implementation that can handle inter-subject variability.
\end{abstract}

\begin{IEEEkeywords}
SSVEP, BCI, Machine Learning, Filter Bank Canonical Correlation Analysis, Classification, Inter-subject Variability
\end{IEEEkeywords}

\section{Introduction}
Steady-State Visual Evoked Potentials (SSVEPs) are brain responses elicited by flickering visual stimuli at specific frequencies. When a subject focuses on a stimulus flickering at a particular frequency, the brain produces electrical activity at the same frequency and its harmonics, which can be detected in electroencephalogram (EEG) recordings \cite{vialatte2010steady}. This property makes SSVEP a valuable approach for Brain-Computer Interfaces (BCIs), as different commands can be associated with different flicker frequencies \cite{wang2006practical}.

Traditional SSVEP detection methods rely on signal processing techniques such as power spectral density analysis, canonical correlation analysis (CCA), and filter bank approaches (FBCCA) \cite{chen2015filter}. These methods directly compare the frequency components of EEG signals with reference sine-cosine signals at target frequencies. While effective for many subjects, these approaches often struggle with inter-subject variability, where some individuals produce weaker or atypical SSVEP responses \cite{allison2010toward}.

This paper investigates the performance gap between direct frequency detection methods and machine learning approaches trained on combined datasets from multiple subjects. We demonstrate that a combined machine learning model can significantly outperform traditional FBCCA for subjects with atypical SSVEP responses, achieving perfect classification where direct methods fail.

\section{Methods}

\subsection{Experimental Setup}
Data was collected from two subjects using an 8-channel EEG system with electrodes placed at standard occipital positions (PO7, PO3, POz, PO4, PO8, O1, Oz, O2). Subjects were presented with four visual stimuli flickering at different frequencies: 15Hz (top), 12Hz (right), 10Hz (bottom), and 9Hz (left). Each experimental session consisted of 20 trials (5 repetitions of each frequency) with a 3-second stimulus presentation period.

\subsection{Datasets}
Four datasets were used in this study:
\begin{itemize}
    \item Subject 1, Training 1
    \item Subject 1, Training 2
    \item Subject 2, Training 1
    \item Subject 2, Training 2
\end{itemize}

Each dataset consisted of continuous EEG recordings at 256Hz sampling rate, with a trigger channel indicating stimulus presentation periods.

\subsection{Signal Preprocessing}
Raw EEG signals were preprocessed using:
\begin{itemize}
    \item Bandpass filtering (4-45Hz)
    \item Notch filtering at 50Hz and 100Hz to remove power line noise
    \item Epoch extraction around trigger events (3-second windows)
    \item Channel selection focusing on occipital electrodes
\end{itemize}

\subsection{Classification Methods}

\subsubsection{Filter Bank Canonical Correlation Analysis (FBCCA)}
FBCCA enhances traditional CCA by decomposing the signal into multiple frequency bands and calculating the canonical correlation between each band and reference sine-cosine signals at the target frequencies. The weighted sum of these correlations is used for classification. Our implementation used:
\begin{itemize}
    \item 8 filter banks with increasing lower cutoff frequencies
    \item 5 harmonics in reference signals
    \item Weighted combination with -1.5 exponential decay
\end{itemize}

\subsubsection{Combined Machine Learning Approach}
Our machine learning approach consisted of:
\begin{itemize}
    \item Feature extraction:
    \begin{itemize}
        \item FBCCA correlations with reference signals for each target frequency
        \item Power spectral density features using Welch's method
        \item Inter-channel correlation features capturing spatial relationships
        \item Temporal features from sliding window analysis
    \end{itemize}
    \item Feature standardization using StandardScaler to normalize across subjects
    \item Model training and selection:
    \begin{itemize}
        \item Support Vector Machine (RBF kernel, C=10, gamma='scale')
        \item Random Forest (100 estimators)
        \item Neural Network (hidden layers: 100, 50 nodes)
        \item Selection of best performer based on training accuracy
    \end{itemize}
    \item Training on combined data from all four datasets with a step size of 0.5 seconds
\end{itemize}

\subsection{Evaluation Methods}
We evaluated classification performance using:
\begin{itemize}
    \item Continuous classification with sliding windows (3-second window, 0.25-second step)
    \item Confusion matrices for each dataset and method
    \item Class-specific accuracy metrics
    \item 10-fold cross-validation to assess generalization performance
    \item Total of 360 samples per dataset (90 samples per frequency class)
\end{itemize}

\section{Results}

\subsection{FBCCA Performance}
The direct FBCCA method showed variable performance across subjects:
\begin{itemize}
    \item Subject 1, Training 1: 95.8\% accuracy
    \item Subject 1, Training 2: 97.2\% accuracy
    \item Subject 2, Training 1: 72.1\% accuracy
    \item Subject 2, Training 2: 68.5\% accuracy
\end{itemize}

Fig. 1 shows the continuous classification results for Subject 1, Training 1, with direct FBCCA. The visualization illustrates predicted versus expected frequencies, FBCCA correlation values, classification confidence, and correct/incorrect predictions.

Fig. 2 presents the confusion matrix for Subject 2, Training 2, demonstrating the difficulty of the direct FBCCA method in correctly classifying certain frequencies for this subject.

\subsection{Combined Machine Learning Performance}
We trained multiple classifier models on the combined dataset:
\begin{itemize}
    \item Support Vector Machine (SVM): 100\% training accuracy
    \item Random Forest: 99.7\% training accuracy
    \item Neural Network: 99.5\% training accuracy
\end{itemize}

The SVM model with RBF kernel (C=10, gamma='scale') consistently performed best and was selected for final evaluation. When applied to the individual datasets, the model achieved:
\begin{itemize}
    \item Subject 1, Training 1: 100\% accuracy
    \item Subject 1, Training 2: 100\% accuracy
    \item Subject 2, Training 1: 100\% accuracy
    \item Subject 2, Training 2: 100\% accuracy
    \item 98.5\% accuracy on 10-fold cross-validation
    \item 97.3\% accuracy in real-time simulation
\end{itemize}

Fig. 3 shows the continuous classification results using the combined machine learning model for Subject 2, Training 2, demonstrating perfect classification where FBCCA struggled.

Fig. 4 presents a bar chart comparing the performance of both methods across all datasets.

\begin{table}[h]
\caption{Classification Performance Comparison}
\label{tab:performance}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Subject 1} & \textbf{Subject 2} & \textbf{Average} \\
\midrule
FBCCA (Direct) & 95-97\% & 68-72\% & $\sim$83\% \\
SVM Model & 100\% & 100\% & 100\% \\
Random Forest & 99.8\% & 99.6\% & 99.7\% \\
Neural Network & 99.7\% & 99.2\% & 99.5\% \\
Cross-validated SVM & 99.3\% & 97.6\% & 98.5\% \\
\bottomrule
\end{tabular}
\end{table}

\section{Discussion}

\subsection{Inter-subject Variability}
Our results highlight the challenge of inter-subject variability in SSVEP-based BCIs. Subject 2 showed significantly lower classification performance with direct FBCCA methods, despite identical experimental conditions. This finding aligns with previous research on "BCI illiteracy," where 15-30\% of individuals may have difficulty generating distinguishable SSVEP patterns \cite{allison2010toward}.

The dramatic performance improvement when using a combined machine learning approach suggests that Subject 2's SSVEP responses were not inherently weaker, but rather had different characteristics that direct frequency detection methods could not capture. By training on data from both subjects, the machine learning model learned to recognize these subject-specific patterns. Analysis of the confusion matrices revealed that:

\begin{itemize}
    \item Subject 1 had strong, distinct responses for each frequency with minimal misclassification
    \item Subject 2 showed significant overlap between 12Hz and 15Hz responses when using FBCCA
    \item The SVM classifier perfectly separated these overlapping patterns using more complex feature relationships
    \item The feature importance analysis showed inter-channel correlation features were particularly important for Subject 2's classification
\end{itemize}

\subsection{Real-time vs. Offline Processing}
Another interesting finding was the discrepancy between real-time FBCCA classification in the simulator (enhanced\_simulation.py) and offline continuous classification (visualize\_predictions.py). The real-time implementation showed lower accuracy, highlighting the challenges of instantaneous decision-making in BCI applications.

The combined machine learning approach demonstrated more stable performance across both real-time and offline scenarios, suggesting that learned features are more robust to temporal variations than direct frequency correlations. Key factors contributing to this stability include:

\begin{itemize}
    \item Longer window size (3 seconds) providing more context for accurate classification
    \item Feature standardization normalizing individual variations in signal amplitude
    \item The SVM model's ability to handle non-linearly separable patterns through its RBF kernel
    \item Comprehensive feature extraction capturing both spectral and spatial characteristics
\end{itemize}

\subsection{Limitations}
While our combined model achieved perfect classification on the test data, it's important to note several limitations:

\begin{itemize}
    \item We tested primarily on the same datasets used for training. While 10-fold cross-validation showed strong performance (98.5\%), independent test data would provide a more robust evaluation.
    \item The training set included only two subjects. A larger and more diverse subject pool would be needed to validate generalizability.
    \item The current approach requires significant preprocessing and feature extraction, which may be computationally intensive for real-time applications.
    \item The 3-second window size, while effective for accuracy, introduces latency that may impact user experience in interactive BCI applications.
\end{itemize}

Future work should employ proper cross-subject validation to better estimate generalization performance.

\section{Conclusion}
This study demonstrates the significant advantage of machine learning approaches over traditional signal processing methods for SSVEP classification, particularly when handling inter-subject variability. Our key findings include:

\begin{itemize}
    \item Traditional FBCCA methods show strong performance for subjects with typical SSVEP responses (95-97\%) but struggle with atypical patterns (68-72\%)
    \item The SVM model trained on combined datasets achieved perfect classification accuracy (100\%) across all test data
    \item Cross-validation confirmed strong generalization capability (98.5\% accuracy)
    \item Feature importance analysis revealed that spatial relationships between channels were critical for handling atypical SSVEP patterns
    \item The performance gap was most pronounced for subjects with atypical SSVEP responses, demonstrating the value of adaptive learning approaches
\end{itemize}

These results suggest that SSVEP-based BCI systems should incorporate adaptive learning approaches that can accommodate individual differences in neural responses, rather than relying solely on direct frequency detection methods.

Future work should focus on:
\begin{itemize}
    \item Developing transfer learning techniques that can quickly adapt to new subjects with minimal calibration data
    \item Optimizing the feature extraction pipeline for real-time implementation
    \item Exploring end-to-end deep learning approaches that may further improve performance
    \item Testing with a larger and more diverse subject population to ensure broader applicability
\end{itemize}

By addressing these challenges, SSVEP-based BCIs can become more practical for real-world applications across diverse user populations.

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{images/prediction_visualization_subject_1_training_1.png}
\caption{Continuous FBCCA classification for Subject 1, Training 1. The figure shows: (top) predicted vs. expected frequencies, (middle-top) FBCCA correlation values for each target frequency, (middle-bottom) classification confidence, and (bottom) classification accuracy over time.}
\label{fig:sub1_fbcca}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{images/continuous_confusion_matrix_subject_2_training_2.png}
\caption{Confusion matrix for Subject 2, Training 2 using direct FBCCA classification. The matrix shows classification accuracy for each of the four target frequencies (15Hz, 12Hz, 10Hz, 9Hz), with notable misclassifications.}
\label{fig:sub2_confmat}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{images/combined_prediction_subject_2_training_2.png}
\caption{Continuous classification for Subject 2, Training 2 using the combined machine learning model. The figure demonstrates perfect classification of all frequency targets.}
\label{fig:sub2_ml}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3.5in]{images/combined_classification_accuracy.png}
\caption{Classification accuracy comparison between FBCCA and the combined machine learning model across all datasets. The machine learning approach achieved perfect accuracy for both subjects.}
\label{fig:accuracy_comparison}
\end{figure}

\bibliographystyle{IEEEtran}
\begin{thebibliography}{00}
\bibitem{vialatte2010steady} F. Vialatte, M. Maurice, J. Dauwels, and A. Cichocki, "Steady-state visually evoked potentials: Focus on essential paradigms and future perspectives," Progress in Neurobiology, vol. 90, no. 4, pp. 418–438, 2010.
\bibitem{wang2006practical} Y. Wang, X. Gao, B. Hong, C. Jia, and S. Gao, "Brain-computer interfaces based on visual evoked potentials," IEEE Engineering in Medicine and Biology Magazine, vol. 27, no. 5, pp. 64-71, 2008.
\bibitem{chen2015filter} X. Chen, Y. Wang, S. Gao, T.-P. Jung, and X. Gao, "Filter bank canonical correlation analysis for implementing a high-speed SSVEP-based brain–computer interface," Journal of Neural Engineering, vol. 12, no. 4, p. 046008, 2015.
\bibitem{allison2010toward} B. Z. Allison, C. Brunner, V. Kaiser, G. R. Müller-Putz, C. Neuper, and G. Pfurtscheller, "Toward a hybrid brain–computer interface based on imagined movement and visual attention," Journal of Neural Engineering, vol. 7, no. 2, p. 026007, 2010.
\end{thebibliography}

\end{document} 